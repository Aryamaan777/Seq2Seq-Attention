# Seq2Seq-Attention

Machine Translation using a Sequence model. Adding Attention to improve results.

Added some notes to help improve understanding of what Attention is and how to implement it.

Why was torch.bmm removed? Why do we use torch.einsum instead?
Any advantages or is torch.bmm deprecated.
